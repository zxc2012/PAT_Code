# 2 Deep Learning
## Neural Network
Deep = Many hidden layers

Last layer still logistic regression

![20230517213033](https://raw.githubusercontent.com/zxc2012/image/main/20230517213033.png)

### Perceptron

$o = f(\sum_{k=1}^n i_kW_k)$

f is an activation function
- Identity: Linear Regression
- Sigmoid: Logistic Regression

One can stack perceptron to a Multi-layer perceptron(MLP)

### Activation functions

![20230517220519](https://raw.githubusercontent.com/zxc2012/image/main/20230517220519.png)

### Backpropagation

- One forward pass: Compute all the intermediate output values
- One backward pass: Follow the chain rule to calculate the gradients at each layer, the calculate the product

Chain rule:

![20230518200950](https://raw.githubusercontent.com/zxc2012/image/main/20230518200950.png)

## CNN

Why CNN for image:
- Some patterns are much smaller than the whole image
- The same patterns appear in different regions
- Subsampling the pixels will not change the object

![20230518210422](https://raw.githubusercontent.com/zxc2012/image/main/20230518210422.png)

### Convolution Layer
Convolutional Filters

![20230518202812](https://raw.githubusercontent.com/zxc2012/image/main/20230518202812.png)

padding: Preserve input spatial dimensions in output activations by padding with n pixels border

![20230518202953](https://raw.githubusercontent.com/zxc2012/image/main/20230518202953.png)

Input: $W * H * D$ tensor

Hyperparameters:
- Number of filters: K, K is usually a power of 2 (32, 64, ...)
- Filter size: $F * F * D$, F is usually an odd number (e.g., 3, 5, 7)
- Stride: S, S describes how we move the filter
- Padding size: P

Output: $W’ * H’ * K$ tensor
- W’ = (W – F + 2P) / S + 1
- H’ = (H – F + 2 P) / S + 1

One can stack convolution filters into a new tensor

![20230518203932](https://raw.githubusercontent.com/zxc2012/image/main/20230518203932.png)

### Pooling Layer

We can use Pooling as Subsampling


Input: $W * H * D$ tensor

Hyperparameters: 
- Filter size: $F * F$
- Stride: S

Output: $W' * H' * D$ tensor
- W' =  (W – F) / S + 1
- H’ = (H – F) / S + 1

### Fully Connected Layer

Flatten

![20230518210350](https://raw.githubusercontent.com/zxc2012/image/main/20230518210350.png)

FC layer contains neurons that connect to the entire input volume, as in ordinary Neural Networks

![20230518205949](https://raw.githubusercontent.com/zxc2012/image/main/20230518205949.png)

## Overfitting

![20230515204344](https://raw.githubusercontent.com/zxc2012/image/main/20230515204344.png)
### Early Stop

![20230517213426](https://raw.githubusercontent.com/zxc2012/image/main/20230517213426.png)

### Weight regularization
#### L1 norm

Objective = $\alpha |\theta|$, we call it Lasso Regression

Gradient descent: More zeros in weights

#### L2 norm

Objective = $\beta |\theta|^2$, we call it Ridge Regression

Decay in weights: $\theta_{t+1} = \theta_{t} - \eta \nabla_{\theta} L(\theta_t)= (1-\lambda)\theta - \eta \nabla_{\theta} L(\theta_t)$

![20230515204304](https://raw.githubusercontent.com/zxc2012/image/main/20230515204304.png)

### Dropout layer

Say dropout rate p.

During training, delete some intermediate output value with probability p or **the weights times 1-p**

![20230517213320](https://raw.githubusercontent.com/zxc2012/image/main/20230517213320.png)