# 3 Word Representation
## Language Model
$$P(w_{1:n}) = \prod_{i = 1}^nP(w_i|w_{1:i-1})
$$

N-gram: distribution of next word is a categorical conditioned on previous N-1 words, $P(w_i|w_{1:i-1}) = P(w_i|w_{i-N+1:i-1})$

I visited San ____

- Unigram: mutual indepedent
- Bigram: P(w|San) 
- 3-gram: P(w|visited San)

Smoothing
- Add-1 estimate(Laplace Smoothing): $P_{\text{Laplace}}(W_n \mid W_{n-1}) = \frac{C(w_{n-1}w_n) + 1}{\sum_w (C(w_{n-1}w) + 1)} = \frac{C(w_{n-1}w_n) + 1}{C(w_{n-1}) + V}$
- Backoff: use trigram if you have good evidence, otherwise bigram, otherwise unigram
- Interpolation: mix unigram, bigram, trigram

Perplexity: inverse probability of the test set, normalized by the number of words

$$PP(WW) = P(w_{1...N})^{-\frac{1}{N}} = \sqrt[N]{\frac{1}{P(w_{1...N})}}$$
## Vector Space

- Term frequency (tf)
- Inverse document frequency (idf)

$t_{ij} = \text{tf}_{ij} * \text{idf}_{j} = \left( \frac{f_{ij}}{m_i} \right) * \left( \log_2 \left( \frac{n}{n_j} \right) + 1 \right)$

where $m_i = \max(f_{ij})$

## Word Embedding

Problems with wordnet
- Requires human labor to create and adapt
- Impossible to keep up-to-date
- Canâ€™t be used to accurately compute word similarity

![20230609205016](https://raw.githubusercontent.com/zxc2012/image/main/20230609205016.png)

### Word2Vec

![20230612204858](https://raw.githubusercontent.com/zxc2012/image/main/20230612204858.png)

Hierarchical Softmax

- Matmul + softmax over |V| (# of words) is very slow to compute for CBOW and SG
- Huffman encode vocabulary, use binary classifiers	to decide which branch to take: log(|V|)

![20230613113255](https://raw.githubusercontent.com/zxc2012/image/main/20230613113255.png)

### Golve

Construct a Global Vectors for Word Representation

Efficiency

- Pros: Efficient for large corpora
- Cons: Relatively slow for small or medium corpora

Effectiveness

- It is a kind of aggregated word2vec/CBOW
    - word2vec mainly focuses on local sliding windows
    - GloVe is able to combine global and local features
- More flexible with the values in matrix
    - log, PMI variants, ... many tricks can be played!