# 3 Word Embeddings
## Language Model
$$P(w_1,w_2,...,w_n) = \prod_{i = 1}^nP(w_i|w_1w_2...w_{i-1})
$$

if $P(w_i|w_1w_2...w_{i-1}) = P(w_i|w_{i-N+1}...w_{i-1})$, we call it N-gram

- Unigram: mutual indepedent
- Bigram: $P(w_i|w_{i-1})$ 

## Word2Vec

Problems with wordnet
- Requires human labor to create and adapt
- Impossible to keep up-to-date
- Canâ€™t be used to accurately compute word similarity

![20230609205016](https://raw.githubusercontent.com/zxc2012/image/main/20230609205016.png)

Vector Semantics

![20230612204858](https://raw.githubusercontent.com/zxc2012/image/main/20230612204858.png)