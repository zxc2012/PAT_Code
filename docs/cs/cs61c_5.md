# CS61C Lecture5 --Parallelism

## Basics

![](https://raw.githubusercontent.com/zxc2012/image/main/20220428200039.png)

### Amdahl’s Law

![](https://raw.githubusercontent.com/zxc2012/image/main/20220428200528.png)

Speedup =$\frac{t_{prev}}{t_{cur}} = \frac{1}{(1-F)+F/S}$

F = Fraction of execution time speed up

S = Scale of improvement

## SIMD

### SIMD Architecture

![](https://raw.githubusercontent.com/zxc2012/image/main/20220505211439.png)

X86 Intrinsics AVX Data Type

![](https://raw.githubusercontent.com/zxc2012/image/main/20220505211808.png)

Intrinsics AVX Code nomenclature [ˈnomənˌkletʃɚ]`name call`

![](https://raw.githubusercontent.com/zxc2012/image/main/20220505212039.png)

![](https://raw.githubusercontent.com/zxc2012/image/main/20220505212550.png)

### Example

#### Matrix Multiply

![](https://raw.githubusercontent.com/zxc2012/image/main/20220505213302.png)

![](https://raw.githubusercontent.com/zxc2012/image/main/20220505214347.png)

```cpp
#include <stdio.h>
// header file for SSE4.2 compiler intrinsics
#include <nmmintrin.h>
// NOTE: vector registers will be represented in
comments as v1 = [ a | b]
// where v1 is a variable of type __m128d and
a,b are doubles
int main(void) {
    // allocate A,B,C aligned on 16-byte boundaries
    double A[4] __attribute__ ((aligned (16)));
    double B[4] __attribute__ ((aligned (16)));
    double C[4] __attribute__ ((aligned (16)));
    int lda = 2;
    int i = 0;
    // declare a couple 128-bit vector variables
    __m128d c1,c2,a,b1,b2;
    A[0] = 1.0; A[1] = 0.0; A[2] = 0.0; A[3] = 1.0;
    B[0] = 1.0; B[1] = 2.0; B[2] = 3.0; B[3] = 4.0;
    C[0] = 0.0; C[1] = 0.0; C[2] = 0.0; C[3] = 0.0;

    c1 = _mm_load_pd(C+0*lda);
    c2 = _mm_load_pd(C+1*lda);
    for (i = 0; i < 2; i++) {

        a = _mm_load_pd(A+i*lda);

        b1 = _mm_load1_pd(B+i+0*lda);
        b2 = _mm_load1_pd(B+i+1*lda);

        c1 = _mm_add_pd(c1,_mm_mul_pd(a,b1));
        c2 = _mm_add_pd(c2,_mm_mul_pd(a,b2));
    }
    // store c1,c2 back into C for completion
    _mm_store_pd(C+0*lda,c1);
    _mm_store_pd(C+1*lda,c2);

    printf("%g,%g\n%g,%g\n",C[0],C[2],C[1],C[3]);
    return 0;
}
```

#### Loop Unrolling

Loop Unrolling in C
```cpp
for(i=0; i<1000; i++)
    x[i] = x[i] + s;
//Unrolling
for(i=0; i<1000; i=i+4) {
    x[i] = x[i] + s;
    x[i+1] = x[i+1] + s;
    x[i+2] = x[i+2] + s;
    x[i+3] = x[i+3] + s;
}
```

RISCV
```armasm
Loop:
lw t0, 0(s0)
addu t0,t0,s1 # add b to array element
sw t0,0(s0) # store result
addi s0,s0,4 # move to next element
bne s0,s2,Loop # repeat Loop if not done

Unrolling Loop:
lw t0,0(s0)
lw t1,4(s0)
lw t2,8(s0)
lw t3,12(s0) # 4 wide SIMD Load
add t0,t0,s1
add t1,t1,s1
add t2,t2,s1
add t3,t3,s1 # 4 wide SIMD Add
sw t0,0(s0)
sw t1,4(s0)
sw t2,8(s0)
sw t3,12(s0) # 4 wide SIMD Store
addi s0,s0,16
bne s0,s2,Loop
```

## MIMD

### OpenMP
#### Sychronization

Reduction: specifies that 1 or more variables that are private to each thread are subject of *reduction* operation at **end** of parallel region

```cpp
double compute_sum(double *a, int a_len) {
    double sum = 0.0;
    #pragma omp parallel for reduction(+ : sum)
    for (int i = 0; i < a_len; i++) {
        sum += a[i];
    }
    return sum;
}
```

#### Pitfalls

- Data dependencies
    ```cpp
    a[0] = 1;
    for(i=1; i<5000; i++)
        a[i] = i + a[i-1];
    ```
- Sharing Issues
    ```cpp
    // Problem
    #pragma omp parallel for //Each thread accesses different elements of a, b,and c, but the same temp
    for(i=0; i<n; i++){
        temp = 2.0*a[i];
        a[i] = temp;
        b[i] = c[i]/temp;
    } 
    // Correct
    #pragma omp parallel for private(temp)
    for(i=0; i<n; i++){
        temp = 2.0*a[i];
        a[i] = temp;
        b[i] = c[i]/temp;
    }
    ```
- Updating Shared Variables Simultaneously 

    This can be done by surrounding the summation by a critical/atomic section or reduction clause

- Parallel Overhead

    Parallelize over the largest loop that you can (even though it will involve more work to declare all of the private variables and eliminate dependencies)

## Cache Coherence